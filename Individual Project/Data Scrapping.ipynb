{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# List of company symbols (or names) to scrape news for\n",
    "companies = ['META', 'AAPL', 'GOOG', 'AMZN', 'MSFT']  # Add other company symbols/names as needed\n",
    "\n",
    "# Set up Chrome options for Selenium\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Path to your ChromeDriver (update the path)\n",
    "# webdriver_service = Service('path/to/chromedriver')  # Replace with the correct path to chromedriver\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Set to store unique articles\n",
    "articles_seen = set()\n",
    "article_data = []  # List to store article data\n",
    "\n",
    "# Function to scroll to the bottom of the page\n",
    "def scroll_to_bottom(max_scrolls):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_count = 0  # Initialize scroll counter\n",
    "\n",
    "    while scroll_count < max_scrolls:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new articles to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            break  # Exit if no new height detected\n",
    "        \n",
    "        last_height = new_height\n",
    "        scroll_count += 1  # Increment scroll counter\n",
    "\n",
    "# Loop through each company and scrape the data\n",
    "for company in companies:\n",
    "    # Construct the URL for the current company\n",
    "    url = f'https://finance.yahoo.com/quote/{company}/news/'\n",
    "    \n",
    "    # Navigate to the page\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load completely\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Scroll to the bottom of the page to load all articles\n",
    "    scroll_to_bottom(10)\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all article sections\n",
    "    articles = soup.select('article section:nth-of-type(2) div div div div ul li')\n",
    "\n",
    "    # Loop through articles and extract titles, links, and details\n",
    "    for article in articles:\n",
    "        title_element = article.select_one('section div a h3')\n",
    "        link_element = article.select_one('section div a')\n",
    "        body_element = article.select_one('section div a p')  # For the short body\n",
    "        time_element = article.select_one('.publishing')  # Publishing time or additional info\n",
    "\n",
    "        # Ensure title, link, and body are found\n",
    "        if title_element and link_element and body_element:\n",
    "            title = title_element.get_text()\n",
    "            link = link_element['href']\n",
    "            body = body_element.get_text()\n",
    "            additional_info = time_element.get_text(strip=True).split('â€¢')[-1].strip() if time_element else None\n",
    "\n",
    "            # Ensure the link is absolute\n",
    "            if not link.startswith('http'):\n",
    "                link = 'https://finance.yahoo.com' + link\n",
    "\n",
    "            # Avoid duplicates based on title\n",
    "            if title not in articles_seen:\n",
    "                articles_seen.add(title)  # Track seen titles\n",
    "                article_data.append({\n",
    "                    'Title': title,\n",
    "                    'Link': link,\n",
    "                    'Short Body': body,\n",
    "                    'Additional Info': additional_info,\n",
    "                    'Company': company  # Add the company name to the article data\n",
    "                })\n",
    "\n",
    "            # Stop when reaching 800 unique articles (adjust as needed)\n",
    "            if len(articles_seen) >= 800:\n",
    "                break\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(article_data)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data scraping completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df  \n",
    "\n",
    "def parse_date_and_time(additional_info):\n",
    "    now = datetime.now()\n",
    "   \n",
    "    if 'hour' in additional_info:\n",
    "        hours = int(additional_info.split()[0])\n",
    "        posted_date = now - timedelta(hours=hours)\n",
    "    \n",
    "    elif 'yesterday' in additional_info:\n",
    "        posted_date = now - timedelta(days=1)\n",
    "    \n",
    "    elif 'day' in additional_info:\n",
    "        days = int(additional_info.split()[0])\n",
    "        posted_date = now - timedelta(days=days)\n",
    "    \n",
    "    else:\n",
    "        posted_date = now\n",
    "    \n",
    "    return posted_date.date(), posted_date.strftime('%H:%M:%S')\n",
    "\n",
    "df_copy[['Date', 'Time']] = df_copy['Additional Info'].apply(lambda x: pd.Series(parse_date_and_time(x)))\n",
    "df_copy.drop('Posted Date',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.to_csv(\"yahoo_finance_articles.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
